\documentclass[Main.tex]{subfiles}
\begin{document}
\section{Conclusie}
In deze conclusie zullen de hypotheses kort overlopen worden. Eerst overloopt de conclusie de subhypotheses om vervolgens te eindigen met de hoofdhypothese.

\subsubsection{Er zijn redundante knopen in de originele boom}
Zoals verwacht zijn redundante knopen inderdaad aanwezig in de originele boom. Het aantal redundante knopen neemt toe naarmate de bewerkingsboom verder wordt uitgewerkt. Een logisch gevolg van het feit dat absoluut redundante knopen ervoor zorgen dat alle kinderen ook redundant zijn. Als er gekeken wordt naar het procentueel aantal redundante knopen per niveau in de bewerkingboom, dit stagneert rond 35\% redundantie.

\subsubsection{Toevoeging van constanten heeft een postieve invloed}
Het toevoegen van constanten verhoogt de oplossingsgraad. De oplossingsgraad stijgt namelijk van 52\% naar 97\%. Dit is een gevolg van de gebruikte randomgenerator. Deze voorziet namelijk een willekeurig gekozen aantal constanten (aantal = 0, 1 of 2). Verwacht wordt dat de gebruiker zulke vergelijkingen zoekt. Denk maar bijvoorbeeld aan de vergelijking $D = B^{2} - 4 \ast A \ast C$. Het nadeel van het toevoegen van constanten is dat de zoektijd toeneemt en er vaak minder diep in de bewerkingsboom kan gezocht worden binnen de beperkte tijd. Dit probleem wordt besproken in de volgende subhypothese.

\subsubsection{Er bestaat een goede afweging tussen de oplossingsgraad en de benodigde tijd bij het toevoegen van constanten}
De zoektijd neemt toe naarmate er meer constanten worden toegevoegd. Dit is volgt uit het feit dat de uitwerking van de bewerkingsboom breder wordt. De bedoeling is dus enerzijds het aantal constanten te beperken en anderzijds effici\"ent om te gaan met deze constanten. Het eerste wordt bereikt door enkel priemgetallen toe te laten kleiner dan tien. Uit experimenten bleek dat de getallen $(1, 2, 3, 5, 7)$ de hoogste oplossingsgraad leveren zonder de tijd al te veel te laten toenemen. Een mogelijke verklaring hiervoor is dat met deze constanten ook makkelijk andere constanten gemaakt kunnen worden. Het tweede wordt bereikt door de neutrale elementen van bewerkingen niet toe te laten. Zo wordt er vermeden dat er op vergelijkingen zoals $1*a$ wordt verdergebouwd. (zo wordt bv $1*a*b$ nooit berekend). Ook is het niet er niet toegelaten dat er met constanten andere bestaande constanten kunnen worden gegenereerd. Bijvoorbeeld $1+2$ is niet toegelaten aangezien $3$ reeds bestaat. Deze beperkingen lijken klein maar naarmate de bewerkingsboom verder wordt uitgewerkt vergroot de besparing exponentieel.

\subsubsection{Het vermijden van redundante uitwerkingen heeft een positieve invloed op de tijd}
Ook hier kan de hypothese positief beantwoord worden. Dit komt door het feit dat rendundante knopen op een effici\"ente manier worden verwijderd. Er is gekozen om bepaalde redundanties niet te verwijderen omdat het controleren hierop meer tijd vraagt dan de verdere uitwerking van deze knopen. Anderzijds is er gekozen om een veralgemenisering van bepaalde redundanties te gebruiken. Specifieke gevallen hiervan vragen te veel tijd om deze weg te werken. Het gevolg hiervan is dat de oplossingsgraad daalt, maar de zoektijd enorm verminderd. Doordat de zoektijd kleiner is, kunnen er meer vergelijkingen worden bekeken binnen dezelfde tijd. Dit zorgt ervoor dat de oplossingsgraad uiteindelijk toch een lichte stijging ondervindt.

\subsubsection{Er kan een passende vergelijking gevonden worden binnen een beperkte tijdspanne.}
Dit is de hoofdhypothese. Enerzijds moet er gemeld worden dat voor vergelijkingen van lengte vijf of kleiner bv. $B^{2} - 4 \ast A \ast C$ er in $87\%$ van de gevallen een oplossingen gevonden wordt. De vergelijkingen die niet gevonden worden bevatten vaak \'e\'en constante die niet tussen de gekozen constanten zit. Het gevolg hiervan is dat de vergelijking pas op een niveau dieper in de boom gevonden zal worden. Dit niveau wordt door de beperkte zoektijd niet bereikt. Anderzijds is het resultaat teleurstellend. De oplossingsgraad van het brute force algoritme ligt namelijk op $83\%$ en verschilt dus niet veel van de effi\"ente manier. Dit valt te verklaren door het feit dat zowel het brute force algoritme en het effici\"entere algorimte binnen de twee seconden ongeveer dezelfde zoekruimte afgaan. Het verschil tussen de twee algoritmes is pas groot op verdere niveaus. Jammer genoeg zal naar de gebruiker toe dit verschil dus amper merkbaar zijn. De eindconclusie is: "Het mogelijk is om vergelijkingen van beperkte lengte te vinden binnen een beperkte tijd van twee seconden. De oplossingsgraad van het effici\"ente algoritme zal echter niet veel verschillen van brute force algoritme".
\end{document}