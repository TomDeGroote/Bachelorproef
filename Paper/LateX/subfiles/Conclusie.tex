\documentclass[Main.tex]{subfiles}
\begin{document}
\section{Conclusie}
In deze conclusie zullen de subhypotheses kort overlopen worden om vervolgens te eindigen met de hoofdhypothese.

\subsubsection{Er zijn redundante knopen in de originele boom}
Zoals verwacht zijn er inderdaad redundante knopen aanwezig in de originele boom. Het aantal hiervan neemt toe naarmate de bewerkingsboom verder wordt uitgewerkt. Dit is het logische gevolg van het feit dat de kinderen van absoluut redundante knopen ook redundant zijn. Het aantal redundante knopen per niveau in de bewerkingboom stagneert rond de 35\%.

\subsubsection{Toevoeging van constanten heeft een postieve invloed}
Het toevoegen van constanten verhoogt de oplossingsgraad. Deze stijgt namelijk van 52\% naar 97\%. Dit is een gevolg van de gegenereerde dataset. De randomgenerator gebruikt namelijk een willekeurig gekozen aantal constanten (aantal = 0, 1 of 2). Dit soort van vergelijkingen ligt in het verwachtingspatroon van de gebruiker. Denk maar bijvoorbeeld aan de vergelijking $D = B^{2} - 4 \ast A \ast C$. Het nadeel van het toevoegen van constanten is dat de zoektijd toeneemt en er vaak minder diep in de bewerkingsboom kan gezocht worden binnen de beperkte tijd. Dit probleem wordt besproken in de volgende sectie.

\subsubsection{Er bestaat een goede afweging tussen de oplossingsgraad en de benodigde tijd bij het toevoegen van constanten}
De zoektijd neemt toe naarmate er meer constanten worden toegevoegd. Dit is een gevolg van de verbreding van de bewerkingsboom. De bedoeling is om enerzijds het aantal constanten te beperken en anderzijds om effici\"ent gebruikt te maken deze constanten. Het eerste wordt bereikt door enkel priemgetallen toe te laten kleiner dan tien. Uit experimenten bleek dat de getallen $(1, 2, 3, 5, 7)$ de hoogste oplossingsgraad leveren zonder de tijd al te veel te laten toenemen. Een mogelijke verklaring hiervoor is dat met deze constanten ook makkelijk andere constanten gemaakt kunnen worden. Het tweede wordt bereikt door de neutrale elementen van bewerkingen niet toe te laten. Zo wordt er vermeden dat er op vergelijkingen zoals $1*a$ wordt verdergebouwd. (zo wordt bv $1*a*b$ nooit berekend). Ook is het niet toegelaten dat er met constanten andere bestaande constanten kunnen worden gemaakt. Bijvoorbeeld $1+2$ is niet toegelaten aangezien $3$ reeds bestaat in de set van constanten. Deze beperkingen lijken klein, maar naarmate de bewerkingsboom groeit vergroot de besparing exponentieel.

\subsubsection{Het vermijden van redundante uitwerkingen heeft een positieve invloed op de tijd}
Ook hier kan de subhypothese positief beantwoord worden. Rendundante knopen op een effici\"ente manier verwijderd worden. Er is echter gekozen om bepaalde redundanties niet te verwijderen, omdat het controleren hierop meer tijd vergt dan de verdere uitwerking van deze knopen. Bovendien is er gekozen om een veralgemenisering van bepaalde redundanties te gebruiken. Specifieke gevallen hiervan vragen te veel tijd om verwijderd te worden. Het gevolg is dat de oplossingsgraad daalt, maar de benodigde zoektijd enorm vermindert. Door kleinere zoektijd kunnen er meer vergelijkingen worden bekeken binnen dezelfde tijd. Dit zorgt ervoor dat de oplossingsgraad uiteindelijk toch een lichte stijging ondervindt.

\subsubsection{Er kan een passende vergelijking gevonden worden binnen een beperkte tijdspanne.}
Voor vergelijkingen van lengte vijf of kleiner bv. $B^{2} - 4 \ast A \ast C$ wordt er in $87\%$ van de gevallen een oplossing gevonden. De overige 13\% van de vergelijkingen bevatten vaak \'e\'en constante die niet tussen de gekozen set van constanten zit. Het gevolg hiervan is dat de vergelijking pas op een lagere diepte in de boom gevonden wordt. Door de beperkte zoektijd wordt deze diepte niet bereikt. Ondanks deze 87\% is het resultaat eerder teleurstellend. De oplossingsgraad van het 'brute force algorithm' ligt namelijk op $83\%$ en verschilt dus niet veel van het nieuwe algoritme. Beide algoritmes gaan binnen de twee seconden ongeveer dezelfde zoekruimte af. Het verschil tussen de twee algoritmes is pas groot op hogere diepten. Jammer genoeg zal naar de gebruiker toe dit verschil amper merkbaar zijn. De eindconclusie is: \textbf{"Het is mogelijk binnen een beperkte tijd van 2 seconden vergelijkingen te vinden die van beperkte lengte zijn. De oplossingsgraad van het effici\"ente algoritme verschilt echter niet veel van het bestaande 'brute force algorithm'."}
\end{document}